Index: src/main/java/phase2/QLearning/QStates.java
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>package phase2.QLearning;\nimport java.util.ArrayList;\nimport java.util.Random;\n\npublic class QStates {\n\n    /* S STATE CLASS DESCRIPTION\n    * OUT-CONNECTION: requires instances from Agent to compute their Q table\n    * GOAL: A reinforcement learning method that uses heuristics to associate a grade to a (state; action) pair\n    *       We iterate through t step to make the value converge.\n    *\n    * */\n    private final double alpha=0.1;//learning rate, alpha\n    private int[][] R;//rewards\n    private double[][] Q;\n    private int[][] EM;\n    private int[][] IN; // instruction table TO BE DONE\n    private final double gamma = 0.5;\n    int currentState = 0;\n\n    private boolean reachedGoal = false;\n\n    //Table Updators\n    public void EMupdate(Agent n){\n        for(int i = 0; i < EM.length(); i++){\n            //n.get;\n        }\n    }\n    public void QUpdate(int[][] newQTable){\n\n    }\n\n\n    public int[] rewardTable = {-1,-10,10,-100,100,20,Integer.MIN_VALUE, -100, 1000};\n/* STATIC REWARD TABLE\n\n    We associate an arbitrary grade to a given situation\n\n    0 = private int move = -1;\n    1 = private int wrongPath = -10; // random move with a low probability.\n    2 = private int correctPath = 10;\n    3 = private int death = -100;\n    4 = private int visionOnIntruder = 100;\n    5 = private int hearingOnIntruder = 20;\n    6 = private int wall = Integer.MIN_VALUE;\n    7 = private int startPoint = -100\n    8 = private int goal = 1000;\n\n */\n    private int[][] maze;//init with width and height? is it computationally effective?\n    int mazeWidth = 100;\n    int mazeHeight = 200;\n    private final int numberOfStates = mazeWidth * mazeHeight;\n    private final int numberOfActions = 5;\n    private int finalState=100;//change this, don't know if correct\n\n    /**\n     * necessary general methods\n     * init- kind of for reading file\n     * calculateQ\n     * printQ\n     *printPolicy\n     * look up Q table\n     * decide move;=> update the Agent position\n     */\n\n    public void init() {\n\n        R = new int[numberOfStates][5]; // creation of the reward Table\n        Q = new double[numberOfStates][5];// creation of the Q Table\n        maze = new int[mazeHeight][mazeWidth]; // the whole map, copy of agent (.??.)\n\n\n        for (int k = 0; k < numberOfStates; k++) {\n\n            /**\n             * initiate reward matrix with -1\n             */\n\n            for (int l = 0; l < numberOfActions; l++) {\n\n                R[k][l] = -1; // everything value of -1; representing that each move costs -1, to ensure it takes the shortest path.\n\n                //HOW TO RETRIEVE/SET FINAL STATE?\n                //if not in the final state, or there is no wall ahead, move in all directions\n                //can fill in later, but I will just fill this in for now\n                if (!reachedGoal())\n                    getpath();\n            }\n            initializeQ();\n        }\n    }\n\n\n        /**\n         * below methods for deciding states, possible actions from states\n         */\n\n\n        //Set Q values to R values\n        public void initializeQ ()\n        {\n            for (int i = 0; i < numberOfStates; i++) {\n                for (int j = 0; j < numberOfActions; j++) {\n\n                    // read from map\n\n                }\n            }\n        }\n\n        public void lookupQTable(Agent a){\n            /** return the best move for an agent\n             * by looking for the max value\n             *  Up down, left, right OR Turn left turn right\n             */\n\n        int agentPositionX = a.getCurrentX();\n        int agentPositionY = a.getCurrentY();\n        int currentMax = -100;// set as minimum when tile contains a wall\n\n            int[] maxList = new int[4];\n            //look up left\n           maxList[0] = R[agentPositionX][agentPositionY - 1];\n            //look right\n            maxList[1] = R[agentPositionX][agentPositionY + 1]\n            //look up Up\n            maxList[2] = R[agentPositionX + 1][agentPositionY]\n            //look up Down\n            maxList[3] = R[agentPositionX - 1][agentPositionY]\n\n            //in case no surrounding position is better than the current one: turn\n            for (int i = 0; i < maxList.length; i++){\n                if(currentMax < maxList[i]){\n                    currentMax = maxList[i];//\n                }\n                if(currentMax > -1000){\n                    //update\n                    a.move()\n                else\n                //call turn + or minus : TO DO evaluate the best half average left > average right ?\n\n        }\n    /**\n     * method for running number of training cycles\n     * inital 1000\n     */\n    public void calculateQValues(){\n        Random randomValue = new Random();\n\n\n\n    }\n\n    public boolean decideFinalState(int state){\n        int i = state / mazeWidth;\n        int j = state - i * mazeWidth;\n\n        return maze[i][j]=='F';\n    }\n    /**\n     * define list of next states the agent can turn to, can only exist if value is !=1\n     * tracks index of states that can be reached\n     */\n    public int[] listOfPossibleStates(int state){\n        ArrayList<Integer> possibleStates=new ArrayList<>();\n        for (int i = 0; i < numberOfStates; i++) {\n            if(R[state][i]!=-1){//use tile to refactor this\n                possibleStates.add(i);\n            }\n        }\n        return possibleStates.stream().mapToInt(i -> i).toArray();\n    }\n\n    double maxQvalues(int nextState){\n        int[] actionsFromState = listOfPossibleStates(nextState);\n        double maxValue=-10;//resetting later, set initial as -10 according to our model\n        for(int action:actionsFromState){\n            double value=Q[nextState][action];\n\n            if(value>maxValue){\n                maxValue=value;\n            }\n        }\n        return maxValue;\n\n    }\n\n    /**\n     * tester\n     */\n    void printQValues(){\n        System.out.println(\"Q matrix values\");\n        for (int i = 0; i < Q.length; i++) {\n            System.out.print(\"From state \" + i + \":  \");\n            for (int j = 0; j < Q[i].length; j++) {\n                System.out.println((Q[i][j]));\n            }\n            System.out.println();\n        }\n    }\n\n}\n\n\n\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/src/main/java/phase2/QLearning/QStates.java b/src/main/java/phase2/QLearning/QStates.java
--- a/src/main/java/phase2/QLearning/QStates.java	(revision 2c21b94eade9c84d5f149fc80d8a070e8e453036)
+++ b/src/main/java/phase2/QLearning/QStates.java	(date 1650539434662)
@@ -1,6 +1,7 @@
 package phase2.QLearning;
 import java.util.ArrayList;
 import java.util.Random;
+import phase2.Agent;
 
 public class QStates {
 
@@ -18,6 +19,7 @@
     private final double gamma = 0.5;
     int currentState = 0;
 
+
     private boolean reachedGoal = false;
 
     //Table Updators
